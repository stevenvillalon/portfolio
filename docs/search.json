[
  {
    "objectID": "TIDYTUESDAY/2025-05-20-SYDNEY.BEACHES/2025-05-20_tidy_tuesday_sydney_beaches_O.html",
    "href": "TIDYTUESDAY/2025-05-20-SYDNEY.BEACHES/2025-05-20_tidy_tuesday_sydney_beaches_O.html",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "#Load dependencies\nlibrary(tidyverse)\nlibrary(tidytuesdayR)"
  },
  {
    "objectID": "TIDYTUESDAY/2025-05-20-SYDNEY.BEACHES/2025-05-20_tidy_tuesday_sydney_beaches_O.html#outliers",
    "href": "TIDYTUESDAY/2025-05-20-SYDNEY.BEACHES/2025-05-20_tidy_tuesday_sydney_beaches_O.html#outliers",
    "title": "Water Quality at Sydney Beaches",
    "section": "Outliers",
    "text": "Outliers\n\n# High enterococci\nhead(water_quality[water_quality$enterococci_cfu_100ml &gt; 500, ], 10)\n\n\n  \n\n\n\n\n# High water temp\nhead(water_quality[water_quality$water_temperature_c &gt; 100, ], 10)\n\n\n  \n\n\n\nThese last few code lines indicate that there are quite a few NAs in the dataset.\n\n# Remove NAs in enterococci\nwater_quality_clean &lt;- water_quality |&gt; \n  filter(!is.na(enterococci_cfu_100ml))\nwater_quality_clean\n\n\n  \n\n\n\nThe summary table does not indicate there were any unusual outliers in the weather dataset.\nJoining the weather dataset to the clean water quality dataset.\n\n# Join datasets\ndf &lt;- left_join(water_quality_clean, weather, by = \"date\")\n\n# Get column names\n#colnames(df)\n\nstart_date &lt;- as.Date(\"2015-01-01\")\nend_date &lt;- as.Date(\"2024-12-31\") \n\n# Rename and Remove duplicate columns\n# Remove outliers on water temperature (under 0 is freezing, over 100 is boiling)\n# Limit data to last 10 years\ndf &lt;- df |&gt; \n  select(\"region\", \"council\", \"swim_site\", \"date\", \"time\", \"enterococci_cfu_100ml\", \"water_temperature_c\", \"conductivity_ms_cm\", \"latitude.x\",\"longitude.x\", \"max_temp_C\", \"min_temp_C\", \"precipitation_mm\") |&gt; \n  rename(latitude = latitude.x, longitude = longitude.x) |&gt; \n  filter(between(water_temperature_c, 0, 100),\n         between(date, start_date, end_date))\n\nhead(df)"
  },
  {
    "objectID": "TIDYTUESDAY/2025-05-06-NSF.GRANT.TERMINATIONS/2025-05-06_tidy_tuesday_nsf.grant.terminations_R1.html",
    "href": "TIDYTUESDAY/2025-05-06-NSF.GRANT.TERMINATIONS/2025-05-06_tidy_tuesday_nsf.grant.terminations_R1.html",
    "title": "NSF Grant Terminations",
    "section": "",
    "text": "#Load dependencies\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\n\nLoad Data\n\n# Load data\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 18)\nnsf_terminations &lt;- tuesdata$nsf_terminations\n\n#View(nsf_terminations)\n\n\n\nExplore Data\n\n# Get summary states for each variable\nsummary(nsf_terminations)\n\n  grant_number     project_title      termination_letter_date\n Min.   :2011780   Length:1041        Min.   :2025-04-18     \n 1st Qu.:2152437   Class :character   1st Qu.:2025-04-18     \n Median :2301114   Mode  :character   Median :2025-04-25     \n Mean   :2266765                      Mean   :2025-04-22     \n 3rd Qu.:2342769                      3rd Qu.:2025-04-25     \n Max.   :2520318                      Max.   :2025-04-25     \n                                                             \n   org_name           org_city          org_state         org_district      \n Length:1041        Length:1041        Length:1041        Length:1041       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n usaspending_obligated  award_type        directorate_abbrev directorate       \n Min.   :   6774       Length:1041        Length:1041        Length:1041       \n 1st Qu.: 188976       Class :character   Class :character   Class :character  \n Median : 356236       Mode  :character   Mode  :character   Mode  :character  \n Mean   : 590244                                                               \n 3rd Qu.: 737336                                                               \n Max.   :6000000                                                               \n NA's   :2                                                                     \n   division         nsf_program_name     nsf_url          usaspending_url   \n Length:1041        Length:1041        Length:1041        Length:1041       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n nsf_startdate        nsf_expected_end_date   org_zip         \n Min.   :2021-02-01   Min.   :2025-04-18    Length:1041       \n 1st Qu.:2022-07-15   1st Qu.:2025-07-31    Class :character  \n Median :2023-08-01   Median :2026-04-30    Mode  :character  \n Mean   :2023-04-24   Mean   :2026-07-20                      \n 3rd Qu.:2024-05-01   3rd Qu.:2027-06-30                      \n Max.   :2025-04-01   Max.   :2029-12-31                      \n                                                              \n   org_uei            abstract         in_cruz_list   \n Length:1041        Length:1041        Mode :logical  \n Class :character   Class :character   FALSE:572      \n Mode  :character   Mode  :character   TRUE :469      \n                                                      \n                                                      \n                                                      \n                                                      \n\n\n\n# Count terminations by state\ntable(nsf_terminations$org_state)\n\n\n AK  AL  AR  AZ  CA  CO  CT  DC  DE  FL  GA  HI  IA  ID  IL  IN  KS  KY  LA  MA \n  3  11   6  32 112  39   8  30   3  36  40   8   6   6  42  23   3   7  12  51 \n MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM  NV  NY  OH  OK  OR  PA  PR \n 32   1  41  19  11   8   2  38   3   9   2  13  10   6  69  22   5  11  45   2 \n RI  SC  TN  TX  UT  VA  VI  WA  WI  WV \n 11   6  15  73  13  41   1  36  17   1 \n\n# Count unique states\nlength(unique(nsf_terminations$org_state))\n\n[1] 50\n\n\nIn the dataset provided by TidyTuesday, there were no awards cancelled in Wyoming or South Dakota. The list includes Puerto Rico and Washington D.C., which is why the total is 50.\nI suspect that blue states were disproportionately affected by these orders. Let’s see if that is true.\n\n# Convert table to dataframe\ncancelled_awards &lt;- as.data.frame(table(nsf_terminations$org_state))\ncancelled_awards &lt;- cancelled_awards |&gt; \n  rename(state = Var1,\n         cancelled_awards_cnt = Freq)\nhead(cancelled_awards)\n\n\n  \n\n\n# Bring in additional data\nelection &lt;- read_csv(\"data/election_results.csv\")\nawards_by_state &lt;- read_csv(\"data/awards_by_state.csv\")\n\nhead(election)\n\n\n  \n\n\nhead(awards_by_state)\n\n\n  \n\n\n\n\n# Join datasets\ndf &lt;- left_join(election, awards_by_state, by = \"state\")\ndf &lt;- left_join(df, cancelled_awards, by = \"state\")\ndf &lt;- df |&gt; \n  select(\"state_nm.x\", \"state\", \"electoral_votes\", \"winner\", \"population\", \"cancelled_awards_cnt\", \"awards_funded\") |&gt; \n  rename(\"state_nm\" = \"state_nm.x\") |&gt; \n  mutate(cancelled_awards_cnt = replace_na(cancelled_awards_cnt, 0))\n\n#View(df)\n\n\n# Calculate % of awards cancelled and the index\ndf &lt;- df |&gt; \n  mutate(\n    defund_rate = round(cancelled_awards_cnt / awards_funded, digits = 4),\n    defund_index = round(defund_rate / mean(defund_rate, na.rm = TRUE) * 100, digits = 1))\n\n# Add red/blue flag for winner of each state\ndf &lt;- df |&gt; \n  mutate(state_color = case_when(\n    winner == \"Trump\" ~ \"red\",\n    winner == \"Harris\" ~ \"blue\"),\n    defund_flag = ifelse(defund_index &gt;= 100, \"Over Index\", \"Under Index\")\n  )\n\n# Bring in centroids\nstate_centroids &lt;- usmapdata::centroid_labels(\"states\")\n\n# Join centroids to df\ndf &lt;- left_join(df, state_centroids, by = c(\"state\" = \"abbr\"))\n\n# Create a sf object for mapping purposes\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\ndf_sf &lt;- st_as_sf(df)\n\n#View(df)\n#View(df_sf)\n\n\n# Cross tab of how many states over/under indexed\ndf |&gt; \n  group_by(state_color, defund_flag) |&gt; \n  summarize(cnt = n(), .groups = \"drop\")\n\n\n  \n\n\nmean(df$defund_rate)\n\n[1] 0.08928431\n\n\n\n\nPlot\n\nlibrary(usmap)\nlibrary(sf)\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\n# Load Lato font\nfont_add_google(\"Lato\", \"lato\")\nshowtext_auto()\n\n# Extract the Coordinate Reference System (CRS) for usmap and apply it to the df_sf\nusmap_proj &lt;- usmap_crs()\ndf_sf &lt;- st_set_crs(df_sf, st_crs(usmap_proj))\n\n# Convert defund_flag to a factor\ndf_sf$defund_flag &lt;- factor(df_sf$defund_flag)\n\n# Plot map of U.S.\nplot_usmap(\n  regions = \"states\",\n  data = df_sf,\n  values = \"state_color\") +\n\n  # Plot the over/under index bubbles by explicitly mapping the 'geom' column to the 'geometry' aesthetic\n  geom_sf(data = df_sf,\n          aes(geometry = geom, size = defund_flag),\n          color = \"black\",\n          alpha = 0.6,\n          inherit.aes = FALSE) +\n\n  # Fill colors for election results\n  scale_fill_manual(\n    name = \"2024 Election Winner\",\n    values = c(\"blue\" = \"#3498db\",\"red\" = \"#e74c3c\"),\n    labels = c(\"Harris\", \"Trump\")) +\n\n  # Define bubble sizes\n  scale_size_manual(\n      name = \"% of Grants Terminated\", # Legend title\n      values = c(\"Over Index\" = 4, \"Under Index\" = 1),\n      labels = c(\"Over Index\", \"Under Index\")\n      ) +\n\n    # Plot labels\n  labs(title = \"Were NSF grants more likely to be terminated in blue states in April 2025?\",\n       subtitle = \"Average grant terminations per state was ~9%, so over index states are those where 9% or \\nmore of active NSF grants were defunded.\",\n       caption = \"Chart produced by Steven Villalon for Tidy Tuesday exercise on May 6, 2025\") +\n\n  # Finer details\n  theme(\n    text = element_text(family = \"lato\"),\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\",\n    plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nThis is a clean bit of code to get a usmap object and fill the states with a color for future use.\n\n# Clean map plot example\nlibrary(usmap)\nplot_usmap(\n  regions = \"states\",\n  data = df,\n  values = \"state_color\") +\n  \n  # Fill colors for election results\n  scale_fill_manual(\n    name = \"2024 Election Winner\",\n    values = c(\"blue\" = \"#3498db\",\"red\" = \"#e74c3c\"),\n    labels = c(\"Harris\", \"Trump\")) +\n    \n  # Format election results legend\n  theme(\n    legend.position = \"right\",\n    plot.caption = element_text(hjust = 0))\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "TIDYTUESDAY/2025-05-27-DUNGEONS.N.DRAGONS/2025-05-27_tidy_tuesday_dungeons_and_dragons_O.html",
    "href": "TIDYTUESDAY/2025-05-27-DUNGEONS.N.DRAGONS/2025-05-27_tidy_tuesday_dungeons_and_dragons_O.html",
    "title": "Dungeons and Dragons Monsters",
    "section": "",
    "text": "Show Code\n#Load dependencies\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\n\n\nLoad Data\n\n\nShow Code\n# Load data\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 21)\nmonsters &lt;- tuesdata$monsters\nrm(tuesdata)\n\n#View(monsters)\n\n\n\n\nExamine Data\n\n\nShow Code\nsummary(monsters)\n\n\n     name             category               cr             size          \n Length:330         Length:330         Min.   : 0.000   Length:330        \n Class :character   Class :character   1st Qu.: 0.500   Class :character  \n Mode  :character   Mode  :character   Median : 2.000   Mode  :character  \n                                       Mean   : 4.551                     \n                                       3rd Qu.: 6.000                     \n                                       Max.   :30.000                     \n     type           descriptive_tags    alignment               ac       \n Length:330         Length:330         Length:330         Min.   : 5.00  \n Class :character   Class :character   Class :character   1st Qu.:12.00  \n Mode  :character   Mode  :character   Mode  :character   Median :14.00  \n                                                          Mean   :14.29  \n                                                          3rd Qu.:17.00  \n                                                          Max.   :25.00  \n   initiative          hp              hp_number         speed          \n Min.   :-5.000   Length:330         Min.   :  1.00   Length:330        \n 1st Qu.: 1.000   Class :character   1st Qu.: 18.25   Class :character  \n Median : 2.000   Mode  :character   Median : 52.00   Mode  :character  \n Mean   : 3.148                      Mean   : 86.67                     \n 3rd Qu.: 4.000                      3rd Qu.:119.00                     \n Max.   :20.000                      Max.   :697.00                     \n speed_base_number      str             dex             con       \n Min.   : 5.00     Min.   : 1.00   Min.   : 1.00   Min.   : 8.00  \n 1st Qu.:30.00     1st Qu.:11.00   1st Qu.:10.00   1st Qu.:12.00  \n Median :30.00     Median :16.00   Median :13.00   Median :14.50  \n Mean   :30.88     Mean   :15.38   Mean   :12.83   Mean   :15.18  \n 3rd Qu.:40.00     3rd Qu.:19.00   3rd Qu.:15.00   3rd Qu.:17.00  \n Max.   :60.00     Max.   :30.00   Max.   :28.00   Max.   :30.00  \n      int              wis             cha            str_save     \n Min.   : 1.000   Min.   : 3.00   Min.   : 1.000   Min.   :-5.000  \n 1st Qu.: 2.000   1st Qu.:10.00   1st Qu.: 5.000   1st Qu.: 0.000  \n Median : 7.000   Median :12.00   Median : 8.000   Median : 3.000  \n Mean   : 7.864   Mean   :11.82   Mean   : 9.918   Mean   : 2.676  \n 3rd Qu.:12.000   3rd Qu.:13.00   3rd Qu.:14.000   3rd Qu.: 4.000  \n Max.   :25.000   Max.   :25.00   Max.   :30.000   Max.   :17.000  \n    dex_save         con_save         int_save         wis_save     \n Min.   :-5.000   Min.   :-1.000   Min.   :-5.000   Min.   :-4.000  \n 1st Qu.: 1.000   1st Qu.: 1.000   1st Qu.:-4.000   1st Qu.: 0.000  \n Median : 2.000   Median : 2.000   Median :-2.000   Median : 1.000  \n Mean   : 2.118   Mean   : 2.785   Mean   :-1.094   Mean   : 1.873  \n 3rd Qu.: 3.000   3rd Qu.: 4.000   3rd Qu.: 1.000   3rd Qu.: 3.000  \n Max.   :10.000   Max.   :15.000   Max.   :12.000   Max.   :12.000  \n    cha_save           skills          resistances        vulnerabilities   \n Min.   :-5.00000   Length:330         Length:330         Length:330        \n 1st Qu.:-3.00000   Class :character   Class :character   Class :character  \n Median :-1.00000   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 0.00303                                                           \n 3rd Qu.: 2.00000                                                           \n Max.   :12.00000                                                           \n  immunities            gear              senses           languages        \n Length:330         Length:330         Length:330         Length:330        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  full_text        \n Length:330        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\nShow Code\n# Check for Unique Values\nunique(monsters$type)\n\n\n [1] \"Aberration\"           \"Elemental\"            \"Construct\"           \n [4] \"Monstrosity\"          \"Humanoid\"             \"Plant\"               \n [7] \"Fiend\"                \"Dragon\"               \"Ooze\"                \n[10] \"Fey\"                  \"Giant\"                \"Celestial\"           \n[13] \"Swarm of Tiny Undead\" \"Undead\"               \"Beast\"               \n[16] \"Swarm of Tiny Beasts\"\n\n\nShow Code\nunique(monsters$size)\n\n\n[1] \"Large\"           \"Medium\"          \"Small\"           \"Medium or Small\"\n[5] \"Huge\"            \"Gargantuan\"      \"Tiny\"           \n\n\nShow Code\nunique(monsters$alignment)\n\n\n [1] \"Lawful Evil\"     \"Neutral\"         \"Unaligned\"       \"Lawful Neutral\" \n [5] \"Chaotic Evil\"    \"Neutral Evil\"    \"Lawful Good\"     \"Chaotic Good\"   \n [9] \"Neutral Good\"    \"Chaotic Neutral\"\n\n\n\n\nTidy\n\n\nShow Code\n# Remove columns that won't be necessary\nmonsters_clean &lt;- monsters |&gt; \n  select(-c(descriptive_tags, hp, speed, skills, resistances, vulnerabilities, immunities, gear, senses, languages, full_text))\n\n#View(monsters_clean)\n\n\n\n\nShow Code\nlibrary(fastDummies)\n\n# One-hot encode Size\nmonsters_clean &lt;- dummy_cols(monsters_clean, select_columns = \"size\", remove_first_dummy = TRUE, remove_selected_columns = TRUE)\n\n# One-hot encode Alignment\nmonsters_clean &lt;- dummy_cols(monsters_clean, select_columns = \"alignment\", remove_first_dummy = TRUE, remove_selected_columns = TRUE)\n\n#View(monsters_clean)\n\n\n\n\nShow Code\n# Clean column names\nlibrary(janitor)\nmonsters_clean &lt;- janitor::clean_names(monsters_clean)\n\n\n\n\nShow Code\n# Get means of various attributes grouped by monster type\nmonster_means &lt;- monsters_clean |&gt; \n  group_by(type) |&gt; \n  summarize('Challenge Rating' = mean(cr, na.rm = TRUE),\n            Armor = mean(ac, na.rm = TRUE),\n            'Hit Points' = mean(hp_number, na.rm = TRUE),\n            Speed = mean(speed_base_number, na.rm = TRUE),\n            Strength = mean(str, na.rm = TRUE),\n            Dexterity = mean(dex, na.rm = TRUE),\n            Constitution = mean(con, na.rm = TRUE),\n            Intelligence = mean(int, na.rm = TRUE),\n            Wisdom = mean(wis, na.rm = TRUE),\n            Charisma = mean(cha, na.rm = TRUE),\n  )\n\nmonster_means\n\n\n\n  \n\n\n\n\n\nShow Code\n# Rescale means from 0 to 10\nlibrary(scales)\nmonster_means_rescaled &lt;- monster_means |&gt; \n  mutate(across(where(is.numeric), ~ scales::rescale(., to = c(0, 10))))\n\nmonster_means_rescaled\n\n\n\n  \n\n\n\n\n\nShow Code\n# Get list of types\n#types &lt;- unique(monster_means_rescaled$type)\n#cat(paste0('\"', types, '\"', collapse = \", \"))\n\n# Lookup table for monster icons\nicon_lookup &lt;- tibble(\n  type = c(\"Aberration\", \"Beast\", \"Celestial\", \"Construct\", \"Dragon\", \"Elemental\", \"Fey\", \"Fiend\", \"Giant\", \"Humanoid\", \"Monstrosity\", \"Ooze\", \"Plant\", \"Swarm of Tiny Beasts\", \"Swarm of Tiny Undead\", \"Undead\"),\n  icon = c(\n    \"&lt;img src='misc/images/aberration.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/beast.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/celestial.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/construct.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/dragon.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/elemental.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/fey.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/fiend.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/giant.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/humanoid.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/monstrosity.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/ooze.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/plant.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/swarm_of_beasts.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/swarm_of_undead.jpg' width='25'/&gt;\",\n    \"&lt;img src='misc/images/undead.jpg' width='25'/&gt;\"\n  )\n)\n\nicon_lookup\n\n\n\n  \n\n\n\n\n\nShow Code\n# Pivot longer\nmonster_long &lt;- monster_means_rescaled |&gt; \n  pivot_longer(\n    cols = -type,\n    names_to = \"attribute\",\n    values_to = \"score\"\n  )\n\n# Left join location of icon images\nmonster_long &lt;- left_join(monster_long, icon_lookup, by = \"type\") |&gt; \n  mutate(type_label = paste0(icon, \" \", type))\n\n# Convert type to factor\nmonster_long$type &lt;- factor(monster_long$type)\n\nhead(monster_long)\n\n\n\n  \n\n\n\n\n\nVisualization\n\n\nShow Code\nlibrary(ggtext)\nlibrary(showtext)\n\n# Load font\nfont_add_google(\"Lato\", \"lato\")\nshowtext_opts(dpi = 300)\n\n# Make bar plots faceted by type\nmonster_plot &lt;- ggplot(monster_long, aes(x = attribute, y = score, fill = attribute)) +\n  geom_col(width = 0.7) +\n  facet_wrap(\n    ~ type_label,\n    ncol = 4, nrow = 4\n  ) +\n  scale_fill_viridis_d(option = \"viridis\") +\n  labs(\n    title = \"An attribute profile of the Monsters in Dungeons and Dragons\",\n    subtitle = \"Given the name of the game, Dragons are unsurprisingly the most powerful Monsters in the game. Celestials and \\nFiends are also strong across all attributes.\",\n    x = NULL,\n    y = \"Scaled Score (0–10)\",\n    caption = \"\\n\\nChart produced by Steven Villalon for Tidy Tuesday exercise on May 27, 2025\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(size = 20, face = \"bold\"),\n    plot.caption = element_text(hjust = 0),\n    strip.text = ggtext::element_markdown(size = 13, face = \"bold\"),\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    panel.border = element_rect(colour = \"black\", fill = NA, linewidth = 1),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n  )\n\nmonster_plot\n\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Save plot\nggsave(\"output/tidy_tuesday_dungeons_&_dragons_barplots.png\", plot = monster_plot, bg = \"white\", width = 10, height = 10, dpi = 300)\n\n\n\n\nModeling\nTried principal components and factor analysis to get the number of predictors to a more manageable number. The first few principal components only described 50% of the variation. It would have taken a lot more components to get an explainable chart. Likewise, factor analysis gave warnings that the datapoints were likely to correlated with one another.\nLeaving these below for future reference.\n\n\nShow Code\n# Remove non-numerical columns\npredictors &lt;- monsters_clean  |&gt; \n  select(-c(name, category, type))\n\n# Run PCA\npca &lt;- prcomp(predictors, center = TRUE, scale. = TRUE)\nsummary(pca)\n\n\nImportance of components:\n                          PC1    PC2     PC3    PC4    PC5    PC6     PC7\nStandard deviation     3.3326 1.7954 1.39622 1.2484 1.1920 1.1730 1.07667\nProportion of Variance 0.3471 0.1007 0.06092 0.0487 0.0444 0.0430 0.03623\nCumulative Proportion  0.3471 0.4478 0.50872 0.5574 0.6018 0.6448 0.68104\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.05443 1.03831 1.03055 1.00133 0.98237 0.89069 0.85025\nProportion of Variance 0.03474 0.03369 0.03319 0.03133 0.03016 0.02479 0.02259\nCumulative Proportion  0.71579 0.74948 0.78267 0.81400 0.84416 0.86895 0.89154\n                          PC15    PC16    PC17   PC18    PC19    PC20    PC21\nStandard deviation     0.78273 0.72854 0.72359 0.5572 0.53641 0.49143 0.45131\nProportion of Variance 0.01915 0.01659 0.01636 0.0097 0.00899 0.00755 0.00637\nCumulative Proportion  0.91069 0.92727 0.94364 0.9533 0.96233 0.96988 0.97624\n                          PC22    PC23    PC24    PC25    PC26   PC27    PC28\nStandard deviation     0.40341 0.37444 0.37306 0.29857 0.27379 0.2113 0.19666\nProportion of Variance 0.00509 0.00438 0.00435 0.00279 0.00234 0.0014 0.00121\nCumulative Proportion  0.98133 0.98571 0.99006 0.99284 0.99519 0.9966 0.99779\n                          PC29    PC30    PC31    PC32\nStandard deviation     0.15457 0.14085 0.13057 0.09979\nProportion of Variance 0.00075 0.00062 0.00053 0.00031\nCumulative Proportion  0.99854 0.99916 0.99969 1.00000\n\n\nShow Code\n#pca$rotation\n\n# Sort contributing variables in decreasing order for PC1 and PC2\ntop_PC1 &lt;- sort(abs(pca$rotation[, \"PC1\"]), decreasing = TRUE)\ntop_PC2 &lt;- sort(abs(pca$rotation[, \"PC2\"]), decreasing = TRUE)\n\n# View top 10 contributors\nhead(top_PC1, 10)\n\n\n        cr   wis_save   cha_save        cha  hp_number initiative        con \n 0.2722902  0.2719477  0.2655940  0.2652942  0.2617644  0.2495072  0.2480373 \n  con_save   int_save         ac \n 0.2462558  0.2455558  0.2454615 \n\n\nShow Code\nhead(top_PC2, 10)\n\n\n                 dex                  str             str_save \n           0.3336794            0.3189999            0.3044974 \n alignment_unaligned                  int                  con \n           0.2544162            0.2422776            0.2417245 \n            int_save           size_large size_medium_or_small \n           0.2283471            0.2250583            0.2230876 \n            dex_save \n           0.2135419 \n\n\n\n\nShow Code\n# Factor Analysis - calculate optimal # of factors\nlibrary(psych)\nfa.parallel(predictors, fa = \"fa\")\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  NA \n\n\n\n\nShow Code\n# Factor analysis scores\nfa_result &lt;- fa(predictors, nfactors = 6, rotate = \"varimax\", scores = TRUE)\n\n\nWarning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\nThe estimated weights for the factor scores are probably incorrect.  Try a\ndifferent factor score estimation method.\n\n\nWarning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : An\nultra-Heywood case was detected.  Examine the results carefully\n\n\nShow Code\nprint(fa_result$loadings, cutoff = 0.3)\n\n\n\nLoadings:\n                          MR1    MR2    MR3    MR4    MR5    MR6   \ncr                         0.773  0.474                            \nac                         0.763                                   \ninitiative                 0.861                                   \nhp_number                  0.715  0.506                            \nspeed_base_number                                                  \nstr                        0.444  0.778                            \ndex                        0.372 -0.604                            \ncon                        0.586  0.691                            \nint                        0.878         0.321                     \nwis                        0.726                                   \ncha                        0.910                                   \nstr_save                   0.460  0.733                            \ndex_save                   0.806                                   \ncon_save                   0.611  0.594                            \nint_save                   0.869                                   \nwis_save                   0.870                                   \ncha_save                   0.906                                   \nsize_huge                         0.367                            \nsize_large                                             0.940       \nsize_medium                                    -0.972              \nsize_medium_or_small                     0.751                     \nsize_small                                                   -0.518\nsize_tiny                        -0.482                            \nalignment_chaotic_good                                             \nalignment_chaotic_neutral                                    -0.375\nalignment_lawful_evil                                              \nalignment_lawful_good      0.384                                   \nalignment_lawful_neutral                                           \nalignment_neutral                        0.713                     \nalignment_neutral_evil                                             \nalignment_neutral_good                                             \nalignment_unaligned       -0.585        -0.463                0.352\n\n                 MR1   MR2   MR3   MR4   MR5   MR6\nSS loadings    9.528 3.571 1.735 1.462 1.309 1.101\nProportion Var 0.298 0.112 0.054 0.046 0.041 0.034\nCumulative Var 0.298 0.409 0.464 0.509 0.550 0.585\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "TIDYTUESDAY/2025-04-15-PENGUINS/2025-04-15_tidy_tuesday_penguins.html",
    "href": "TIDYTUESDAY/2025-04-15-PENGUINS/2025-04-15_tidy_tuesday_penguins.html",
    "title": "Base R Penguins",
    "section": "",
    "text": "# Load dependencies\nlibrary(tidyverse)\n\n\n# Load data\npenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv')\n\n\nCleaning\n\n# Check for NAs\ncolSums(is.na(penguins))\n\n    species      island    bill_len    bill_dep flipper_len   body_mass \n          0           0           2           2           2           2 \n        sex        year \n         11           0 \n\n# Remove 11 NAs\npenguins &lt;- na.omit(penguins)\n\n# Convert categorical variables to factors\npenguins &lt;- penguins |&gt; \n  mutate(across(c(species, island, sex), as.factor))\n\n# Add column with filename of penguin image\npenguins &lt;- penguins |&gt; \n  mutate(image = case_when(\n    species == \"Adelie\" ~ \"misc/adelie.jpg\",\n    species == \"Chinstrap\" ~ \"misc/chinstrap.jpg\",\n    species == \"Gentoo\" ~ \"misc/gentoo.jpg\"\n  ))\n\nhead(penguins)\n\n\n  \n\n\n\n\n\nExploration\n\n# Summary stats\nsummary(penguins)\n\n      species          island       bill_len        bill_dep      flipper_len \n Adelie   :146   Biscoe   :163   Min.   :32.10   Min.   :13.10   Min.   :172  \n Chinstrap: 68   Dream    :123   1st Qu.:39.50   1st Qu.:15.60   1st Qu.:190  \n Gentoo   :119   Torgersen: 47   Median :44.50   Median :17.30   Median :197  \n                                 Mean   :43.99   Mean   :17.16   Mean   :201  \n                                 3rd Qu.:48.60   3rd Qu.:18.70   3rd Qu.:213  \n                                 Max.   :59.60   Max.   :21.50   Max.   :231  \n   body_mass        sex           year         image          \n Min.   :2700   female:165   Min.   :2007   Length:333        \n 1st Qu.:3550   male  :168   1st Qu.:2007   Class :character  \n Median :4050                Median :2008   Mode  :character  \n Mean   :4207                Mean   :2008                     \n 3rd Qu.:4775                3rd Qu.:2009                     \n Max.   :6300                Max.   :2009                     \n\n\n\n# Get counts by year\npenguins |&gt; \n  group_by(year) |&gt; \n  summarize(count = n(), .groups = \"drop\")\n\n\n  \n\n\n# Get counts by species and island\npenguins |&gt; \n  group_by(species, island) |&gt; \n  summarize(count = n(), .groups = \"drop\")\n\n\n  \n\n\n# Get counts by species, island, and year\npenguins |&gt; \n  group_by(year, species, island) |&gt; \n  summarize(count = n(), .groups = \"drop\")\n\n\n  \n\n\n# Get counts by species and sex\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(count = n(), .groups = \"drop\")\n\n\n  \n\n\n# Get avgs of numeric variables\npenguins_avg &lt;- penguins |&gt; \n  group_by(species) |&gt; \n  summarize(image = first(image),\n            avg_bill_len = mean(bill_len),\n            avg_bill_dep = mean(bill_dep),\n            avg_flipper_len = mean(flipper_len),\n            avg_body_mass = mean(body_mass))\n\npenguins_avg\n\n\n  \n\n\n\n\n# Plot of bill length and depth by species\nggplot(penguins, aes(x = bill_len, y = bill_dep , color = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# Plot of flipper length and body mass by species\nggplot(penguins, aes(x = flipper_len, y = body_mass , color = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# Plot of bill length and depth by sex\nggplot(penguins, aes(x = bill_len, y = bill_dep , color = sex)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# Plot of flipper length and body mass by sex\nggplot(penguins, aes(x = flipper_len, y = body_mass , color = sex)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# Boxplot of bodymass by sex\nggplot(penguins, aes(x = sex, y = body_mass)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nModeling\n\n# Test/training split\nset.seed(123)\nn &lt;- nrow(penguins)\ntest_index &lt;- sample.int(n, size = round(0.2 * n))\ntrain_data &lt;- penguins[-test_index, ]\ntest_data &lt;- penguins[test_index, ]\n\n\n# Fit decision tree\nlibrary(rpart)\nlibrary(partykit)\ntree &lt;- rpart(species ~ bill_len + bill_dep + flipper_len + body_mass, data = train_data)\n\n# Plot tree\nplot(as.party(tree), \n     main = \"Decision Tree for Penguin Species\", \n     gp = gpar(fontsize = 6))\n\n\n\n\n\n\n\n\n\n# Prune the tree\nplotcp(tree)\n\n\n\n\n\n\n\n\nSince none of the simpler trees (sizes 1 or 2) fall below the dotted line, there’s no obvious benefit to pruning based on the 1-SE rule.\n\n# Predict test data\ntest_data$tree_preds &lt;- predict(tree, newdata = test_data, \"class\")\n\n# Generate confusion matrix\nlibrary(caret)\ncm_tree &lt;- confusionMatrix(test_data$tree_preds, test_data$species, \n                dnn = c(\"predicted\", \"actual\"))\ncm_tree\n\nConfusion Matrix and Statistics\n\n           actual\npredicted   Adelie Chinstrap Gentoo\n  Adelie        28         0      0\n  Chinstrap      5         9      1\n  Gentoo         1         2     21\n\nOverall Statistics\n                                          \n               Accuracy : 0.8657          \n                 95% CI : (0.7603, 0.9367)\n    No Information Rate : 0.5075          \n    P-Value [Acc &gt; NIR] : 7.061e-10       \n                                          \n                  Kappa : 0.788           \n                                          \n Mcnemar's Test P-Value : 0.09647         \n\nStatistics by Class:\n\n                     Class: Adelie Class: Chinstrap Class: Gentoo\nSensitivity                 0.8235           0.8182        0.9545\nSpecificity                 1.0000           0.8929        0.9333\nPos Pred Value              1.0000           0.6000        0.8750\nNeg Pred Value              0.8462           0.9615        0.9767\nPrevalence                  0.5075           0.1642        0.3284\nDetection Rate              0.4179           0.1343        0.3134\nDetection Prevalence        0.4179           0.2239        0.3582\nBalanced Accuracy           0.9118           0.8555        0.9439\n\n\n\n# Create scatterplot with images\n\n# Load dependencies\nlibrary(ggimage)\nlibrary(ggrepel)\nlibrary(showtext)\n\n# Load custom font\nfont_add_google(\"Lato\", \"lato\")\nshowtext_auto()\n\n# Generate plot\npenguin_plot &lt;- ggplot(penguins_avg, aes(x = avg_bill_len, y = avg_flipper_len)) +\n  geom_image(aes(image = image), size = 0.3) +\n  geom_text(aes(label = species), vjust = -5.75, hjust = 0.5, size = 4, fontface = \"bold\") +\n  theme_minimal(base_family = \"lato\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(face = \"plain\", size = 12)\n  ) +\n  labs(\n    title = \"Penguin Species by Average Bill and Flipper Length\", \n    subtitle = \"Decision tree model indicated that these two features were best at distinguishing \\nbetween species. Tree splits showed flipper length greater than 210mm likely Gentoo.\\nSmaller flippers and bill lengths less than 42mm likely Adelie.\",\n    x = \"Bill Length (in mm)\", \n    y = \"Flipper Length (in mm)\"\n  ) +\n  xlim(min(penguins_avg$avg_bill_len) - 5, max(penguins_avg$avg_bill_len) + 5) +\n  ylim(min(penguins_avg$avg_flipper_len) - 5, max(penguins_avg$avg_flipper_len) + 10)\n\npenguin_plot\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tableau.html#sv-cruises",
    "href": "tableau.html#sv-cruises",
    "title": "Tableau Projects",
    "section": "1. SV Cruises",
    "text": "1. SV Cruises\nI recently walked the Camino de Santiago in northern Spain, where I met a fellow pilgrim from Denmark who shared stories about his home and travels through Europe. Ever since, I’ve been dreaming of taking a Scandinavian cruise. For fun, I decided to simulate data for a fictitious cruise company and build an interactive dashboard in Tableau to practice my data visualization skills.\nThis project involved designing relationships using Tableau’s logical data model, developing calculated fields and Level of Detail (LOD) expressions, and creating dynamic KPIs with contextual breakdowns. Along the way, I tackled common data aggregation challenges and improved my ability to structure metrics clearly for executive-level dashboards.\nHopefully, I’ll get to turn this dream vacation into a reality soon!"
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Tidy Tuesday Projects",
    "section": "",
    "text": "Below are some of my Tidy Tuesday projects!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDungeons and Dragons Monsters\n\n\n\nFacet grid\n\nBarplot\n\nR\n\n\n\n\n\n\nMay 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality at Sydney Beaches\n\n\n\nLollipop Plot\n\nR\n\n\n\n\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSeismic Events at Mount Vesuvius\n\n\n\nHeatmap\n\nR\n\n\n\n\n\n\nMay 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNSF Grant Terminations\n\n\n\nMap\n\nR\n\n\n\n\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUse R! Conference\n\n\n\nBarplot\n\nText Analysis\n\nLDA\n\nR\n\n\n\n\n\n\nApr 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFatal Car Crashes on 4/20\n\n\n\nBarplot\n\nR\n\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBase R Penguins\n\n\n\nScatterplot\n\nDecision Tree\n\nR\n\n\n\n\n\n\nApr 15, 2025\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "python.html#histograms-hoops",
    "href": "python.html#histograms-hoops",
    "title": "Python Projects",
    "section": "1. Histograms & Hoops",
    "text": "1. Histograms & Hoops\nDuring the Fall 2024 semester, I took courses in Probability & Statistics and Python programming. I wanted to come up with a project that would help me apply the concepts I was learning in both classes. In Statistics, we covered topics like histograms, probability distributions, and random variables. One day, while watching a basketball game with friends, I realized that the abundance of available basketball data would be a great resource to apply these techniques.\nThis app, built with Python and Streamlit, uses data from the NBA API, and focuses on box scores for all players in the 2023-2024 NBA season. It allows users to explore histograms of various player statistics and also estimate the probability of a player reaching a specific performance threshold, like scoring 30 points in a game. In a future version, I plan to expand the app by adding more variables and using regression models to improve prediction accuracy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steven Villalon, MBA",
    "section": "",
    "text": "I’m a former Product Marketing Manager at AT&T. Currently studying data science at the University of Notre Dame.\nCreated this site to share my Tidy Tuesday work and other data projects.\nBased in Miami, FL.\n#DataScience #RStats #Python #Marketing #Analytics"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Steven Villalon, MBA",
    "section": "Interests",
    "text": "Interests\n\nMarketing\nPricing, Email/SMS, Product\n\n\nCoding\nSQL, R, Python\n\n\nData Visualization\nggplot, matplotlib, seaborn\n\n\nDashboarding\nTableau, Shiny, Streamlit\n\n\nModeling\nLinear Regression, Logistic Regression, Random Forests\n\n\nTime Series and Forecasting\nARIMA, ETS"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Steven Villalon Data Science Portfolio",
    "section": "",
    "text": "McKenna Hall at the University of Notre Dame"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Steven Villalon Data Science Portfolio",
    "section": "About Me",
    "text": "About Me\nI have worked in marketing for over 15 years, including 9 at AT&T. I love all aspects of marketing: advertising, pricing, events, creative development, product strategy. But no matter what area of marketing I have worked in, it always comes back to the data. What happened when we ran that promo? How did traffic change when we implemented the website design? How many people tuned into our show? You can’t answer these questions without understanding your data.\nMarketing and data are intricately connected, which is why I am pursuing a Master’s degree in Data Science at the University of Notre Dame.\nThis site is intended to showcase some of the new skills I am developing.\n\n\nEducation\n\nDuke University: B.A., Psychology\nGeorgia Institute of Technology: M.B.A., Marketing\nUniversity of Notre Dame: M.S., Data Science (expected May 2026)\n\n\n\n\nCertifications\n\nUdacity: Data Analyst Nanodegree\nCoursera: Google Digital Marketing & Ecommerce\n\n\n\n\nPersonal\nI live in Miami with my wife Alejandra and my dog Albert. I’m a huge fan of the Miami Heat (and all the South Florida sports teams), so you will certainly see a few basketball related projects!"
  },
  {
    "objectID": "TIDYTUESDAY/2025-05-13-MOUNT.VESUVIUS/2025-05-13_tidy_tuesday_mount_vesuvius_O.html",
    "href": "TIDYTUESDAY/2025-05-13-MOUNT.VESUVIUS/2025-05-13_tidy_tuesday_mount_vesuvius_O.html",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "",
    "text": "#Load dependencies\nlibrary(tidyverse)\nlibrary(tidytuesdayR)"
  },
  {
    "objectID": "TIDYTUESDAY/2025-05-13-MOUNT.VESUVIUS/2025-05-13_tidy_tuesday_mount_vesuvius_O.html#univariate",
    "href": "TIDYTUESDAY/2025-05-13-MOUNT.VESUVIUS/2025-05-13_tidy_tuesday_mount_vesuvius_O.html#univariate",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "Univariate",
    "text": "Univariate\n\nggplot(data = vesuvius_clean, aes(x = depth_km)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\nggplot(data = vesuvius_clean, aes(x = duration_magnitude_md)) +\n  geom_histogram(binwidth = 0.1)"
  },
  {
    "objectID": "TIDYTUESDAY/2025-05-13-MOUNT.VESUVIUS/2025-05-13_tidy_tuesday_mount_vesuvius_O.html#multivariate",
    "href": "TIDYTUESDAY/2025-05-13-MOUNT.VESUVIUS/2025-05-13_tidy_tuesday_mount_vesuvius_O.html#multivariate",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "Multivariate",
    "text": "Multivariate\n\n# Create scatterplot\nggplot(data = vesuvius_clean, aes(x = depth_km, y = duration_magnitude_md)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n# Cross-tab of events per year\nvesuvius_events_per_year &lt;- vesuvius_clean |&gt; \n  group_by(year) |&gt; \n  summarize(event_cnt = n(), .groups = \"drop\")\n\nvesuvius_events_per_year\n\n\n  \n\n\n# Mean events per year\nmean(vesuvius_events_per_year$event_cnt)\n\n[1] 831.4\n\n\n\n# Cross-tab of events per month\nvesuvius_events_per_mo &lt;- vesuvius_clean |&gt; \n  group_by(year, month(vesuvius_clean$time, label = TRUE, abbr = TRUE)) |&gt; \n  summarize(event_cnt = n(), .groups = \"drop\")\n\ncolnames(vesuvius_events_per_mo) &lt;- c(\"year\", \"month\", \"event_cnt\")\n\nvesuvius_events_per_mo\n\n\n  \n\n\n\n\n# Compute mean events by month\nmean_events_per_mo &lt;- vesuvius_events_per_mo |&gt; \n  group_by(month) |&gt; \n  summarize(avg_events = mean(event_cnt))\n\nmean_events_per_mo"
  },
  {
    "objectID": "TIDYTUESDAY/2025-04-29-useR.CONFERENCE/2025-04-29_tidy_tuesday_user.conference.html",
    "href": "TIDYTUESDAY/2025-04-29-useR.CONFERENCE/2025-04-29_tidy_tuesday_user.conference.html",
    "title": "Use R! Conference",
    "section": "",
    "text": "#Load dependencies\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nThe materials at the following link were very helpful for learning about text analysis, topic modeling, and LDA. https://www.tidytextmining.com/topicmodeling#per-document"
  },
  {
    "objectID": "TIDYTUESDAY/2025-04-29-useR.CONFERENCE/2025-04-29_tidy_tuesday_user.conference.html#tf-idf",
    "href": "TIDYTUESDAY/2025-04-29-useR.CONFERENCE/2025-04-29_tidy_tuesday_user.conference.html#tf-idf",
    "title": "Use R! Conference",
    "section": "TF-IDF",
    "text": "TF-IDF\n\n# Pull the gammas (propensity score for which topic each document id belongs to)\ndoc_topic_gammas &lt;- tidy(user2025_lda, matrix = \"gamma\")\ndoc_topic_gammas\n\n\n  \n\n\n# Get the top topic for each document id\nassigned_topics &lt;- doc_topic_gammas |&gt; \n  group_by(document) |&gt; \n  slice_max(gamma, n = 1, with_ties = FALSE) |&gt; \n  mutate(document = as.double(document)) |&gt; \n  ungroup()\nassigned_topics\n\n\n  \n\n\n# Join back to original dataset\nuser2025_with_topics &lt;- user2025 |&gt; \n  left_join(assigned_topics, by = c(\"id\" = \"document\"))\n\n\n# Unnest words from session descriptions (column = content)\ntidy_words_with_topics &lt;- user2025_with_topics  |&gt;  \n  unnest_tokens(word, content) |&gt; \n  select(id, topic, title, word)\n\n# Remove stop words and lemmatize remaining words (remove plurals, suffixes, etc)\ntidy_words_with_topics_nostop &lt;- tidy_words_with_topics |&gt;\n  anti_join(custom_stop_words, by = \"word\") |&gt; \n  mutate(word = lemmatize_words(word))\n\nhead(tidy_words_with_topics_nostop)\n\n\n  \n\n\n\n\n# Get counts by word and topic\nword_counts &lt;- tidy_words_with_topics_nostop |&gt; \n  count(topic, word, sort = TRUE)\nword_counts\n\n\n  \n\n\n# Compute tf-idfs\ntfidf_words &lt;- word_counts |&gt; \n  bind_tf_idf(word, topic, n)\ntfidf_words\n\n\n  \n\n\n# Filter to top 10 tf-idfs for each topic\ntop_tfidf_words &lt;- tfidf_words |&gt; \n  group_by(topic) |&gt; \n  arrange(desc(tf_idf)) |&gt; \n  slice_head(n = 10) |&gt; \n  ungroup()\n\ntop_tfidf_words"
  },
  {
    "objectID": "TIDYTUESDAY/2025-04-29-useR.CONFERENCE/2025-04-29_tidy_tuesday_user.conference.html#bigrams",
    "href": "TIDYTUESDAY/2025-04-29-useR.CONFERENCE/2025-04-29_tidy_tuesday_user.conference.html#bigrams",
    "title": "Use R! Conference",
    "section": "Bigrams",
    "text": "Bigrams\n\nlibrary(tidytext)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Assuming your dataset is called 'talks' and the text column is 'description'\n\n# Step 1: Tokenize into bigrams\ntalks_bigrams &lt;- user2025 %&gt;%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2)\n\n# Step 2: Separate the bigrams into two words\ntalks_bigrams_separated &lt;- talks_bigrams %&gt;%\n  separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\n# Step 3: Remove stopwords\ndata(\"stop_words\")\ntalks_bigrams_filtered &lt;- talks_bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word)\n\n# Step 4: Count the bigrams\nbigram_counts &lt;- talks_bigrams_filtered %&gt;%\n  count(word1, word2, sort = TRUE)\n\n# Step 5: Visualize Top 15 Bigrams\nbigram_counts %&gt;%\n  top_n(15) %&gt;%\n  mutate(bigram = paste(word1, word2, sep = \" \")) %&gt;%\n  ggplot(aes(x = reorder(bigram, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Top 15 Bigrams in Talks\",\n       x = \"Bigram\",\n       y = \"Frequency\")\n\nSelecting by n"
  },
  {
    "objectID": "TIDYTUESDAY/2025-04-22-CAR.CRASHES/2025-04-22_tidy_tuesday_car.crashes.html",
    "href": "TIDYTUESDAY/2025-04-22-CAR.CRASHES/2025-04-22_tidy_tuesday_car.crashes.html",
    "title": "Fatal Car Crashes on 4/20",
    "section": "",
    "text": "# Load dependencies\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\n\n# Load data\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-22')\nnames(tuesdata)\n\n[1] \"daily_accidents\"     \"daily_accidents_420\"\n\n# Extract tt_data\naccidents &lt;- tuesdata$daily_accidents\naccidents.420 &lt;- tuesdata$daily_accidents_420\n\nhead(accidents)\n\n\n  \n\n\n\n\nCleaning\n\n# Pull all 4/20 rows\naccidents.420[month(accidents.420$date) == 4 & day(accidents.420$date) == 20, ]\n\n\n  \n\n\n# Check for NAs in e420\naccidents.420[is.na(accidents.420$e420), ]\n\n\n  \n\n\n# Pull all rows where e420 is true\naccidents.420[which(accidents.420$e420 == TRUE), ]\n\n\n  \n\n\n\nFor 4/20 specifically, the accidents.420 dataframe has multiple rows. FALSE is before 4:20pm and TRUE is after. Strange to see NAs in this column.\nThere are 13 rows where e420 is NA. I will remove because it’s unclear what they mean. In practice, would go to whomever created the dataset and ask why this is happening.\nLimiting to e420 = TRUE is misleading because there were other fatalities prior to 4:20pm and I don’t think all the people who “celebrate” 4/20 are waiting until 4pm to get high.\nFor simplicity, I’m going to focus on the accidents dataset which only includes fatalities by day.\n\n\nExploration\n\n# Summary stats\nsummary(accidents)\n\n      date            fatalities_count\n Min.   :1992-01-01   Min.   : 47.0   \n 1st Qu.:1998-04-01   1st Qu.:121.0   \n Median :2004-07-01   Median :142.0   \n Mean   :2004-07-01   Mean   :145.1   \n 3rd Qu.:2010-10-01   3rd Qu.:166.0   \n Max.   :2016-12-31   Max.   :299.0   \n\nprint(paste(\"Standard deviation of fatalities = \", sd(accidents$fatalities_count)))\n\n[1] \"Standard deviation of fatalities =  33.2602881853278\"\n\n\n\n# Histogram of Fatalities\nggplot(accidents, aes(x = fatalities_count)) +\n  geom_histogram(binwidth = 25, fill = \"lightblue\", color = \"black\")\n\n\n\n\n\n\n\n\nSlightly right skewed with a mean of 145 fatalities per day. Range of 47 to 299 fatalities.\n\n# Time series plot\nggplot(accidents, aes(x = date, y = fatalities_count)) +\n  geom_line()\n\n\n\n\n\n\n\n# Group by month\naccidents$week &lt;- floor_date(accidents$date, \"week\")\naccidents$month &lt;- floor_date(accidents$date, \"month\")\naccidents$year &lt;- floor_date(accidents$date, \"year\")\n\nhead(accidents)\n\n\n  \n\n\n\n420 is interesting on its own, but I’d like to extend the analysis to look at other “party” holidays to see if fatalities are higher. I can also compare the rate of fatalities to national holidays and non-holiday weekdays and weekends.\nParty Holidays:\n\nSuper Bowl Sunday\nMardi Gras\nSt. Patty’s\n4/20\nCinco de Mayo\nHalloween\nThanksgiving Eve\nNew Year’s Eve\n\nNational Holidays:\n\nNew Year’s Day\nMLK Day\nPresident’s Day\nMemorial Day\nIndependence Day\nLabor Day\nColumbus Day\nVeterans Day\nThanksgiving Day\nChristmas Day\n\nLet’s bring in a dataset I made using ChatGPT. This dataset has indicators for the holiday. This is not a good practice because I can’t replicate the creation of that dataset. Will look for a more programmatic solution for future projects.\nNote that in the code below, I am converting “National Holiday (Observed)” to “National Holiday”. Some National Holidays fall on different dates every year (floating), and sometimes the holiday is observed on a Friday or Monday when the actual day falls on Saturday/Sunday. This is an imperfect solution but makes for a cleaner visualization.\n\n# Load holiday data\nholidays &lt;- read_csv(\"data/holidays.csv\",\n                     col_types = cols(date = col_date(format = \"%m/%d/%y\")))\n\n# Join to accidents dataset\naccidents_with_holidays &lt;- left_join(accidents, holidays, by = \"date\")\n\n# Change \"National Holiday (Observed)\" to \"National Holiday\"\naccidents_with_holidays &lt;- accidents_with_holidays |&gt; \n  mutate(day_type = case_when(\n    day_type == \"National Holiday (Observed)\" ~ \"National Holiday\",\n    TRUE ~ day_type  # Keep the rest unchanged\n  ))\n\n# Convert day_type and holiday to factors\naccidents_with_holidays &lt;- accidents_with_holidays |&gt; \n  mutate(\n    across(c(day_type, holiday), as.factor),\n    day_of_week = factor(day_of_week, \n                         levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \n                                    \"Thursday\", \"Friday\", \"Saturday\"))\n  )\n\nhead(accidents_with_holidays)\n\n\n  \n\n\n\n\n# Summary table of Day Type\navg_fatalities_day_type &lt;- accidents_with_holidays |&gt; \n  group_by(day_type) |&gt; \n  summarize(avg_fatalities = round(mean(fatalities_count),1)) |&gt; \n  arrange(desc(avg_fatalities))\navg_fatalities_day_type\n\n\n  \n\n\n# Summary table of Holiday\navg_fatalities_holiday &lt;- accidents_with_holidays |&gt; \n  group_by(holiday) |&gt; \n  summarize(\n        day_type = first(day_type),\n        avg_fatalities = round(mean(fatalities_count), 1)\n  ) |&gt;\n  na.omit(holiday) |&gt; # removes non-holiday weekdays/weekends\n  arrange(desc(avg_fatalities))\navg_fatalities_holiday\n\n\n  \n\n\n\n\n\nVisualization\n\n# Create side by side Bar Plot\nlibrary(patchwork)\nlibrary(showtext)\n\n# Load font\nfamily &lt;- \"Lato\"\nfont_add(family = family, \n         regular = \"Lato-Regular.ttf\",\n         bold = \"Lato-Bold.ttf\")\nshowtext_auto()\nshowtext_opts(dpi = 300)\n\n# Colors\nmy_colors &lt;- c(\n  \"Non-Holiday Weekday\" = \"#76B7B2\",\n  \"Non-Holiday Weekend\" = \"#F28E2B\",\n  \"National Holiday\"     = \"#4E79A7\",\n  \"Party Holiday\"        = \"#E15759\"\n)\n\n# Reorder factor levels in descending order\navg_fatalities_day_type &lt;- avg_fatalities_day_type |&gt; \n  mutate(day_type = reorder(day_type, avg_fatalities))\n\navg_fatalities_holiday &lt;- avg_fatalities_holiday |&gt; \n  mutate(holiday = reorder(holiday, avg_fatalities))\n\n# Plot 1\np1 &lt;- ggplot(avg_fatalities_day_type, aes(x = avg_fatalities, y = day_type, fill = day_type)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.1f\", avg_fatalities)), hjust = 1.1, color = \"white\", size = 3.5, family = family) +\n  labs(title = \"By Day Type\", x = NULL, y = NULL) +\n  scale_fill_manual(values = my_colors) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    text = element_text(family = family)\n  )\n\n# Plot 2\np2 &lt;- ggplot(avg_fatalities_holiday, aes(x = avg_fatalities, y = holiday, fill = day_type)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.1f\", avg_fatalities)), hjust = 1.1, color = \"white\", size = 3.5, family = family) +\n  labs(title = \"By Holiday\", x = NULL, y = NULL) +\n  scale_fill_manual(values = my_colors) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    text = element_text(family = family)\n  )\n\n# Plot Title, Subtitle, and Caption\nplot_title &lt;- plot_annotation(\n  title = \"Daily Fatal Car Crashes in the U.S. (1992-2016)\",\n  subtitle = \"As a whole, holidays that are associated with heavy drug/alcohol use (like 4/20) did not result in \\nthe highest rate of fatal car crashes. As the authors found in the original study, Independence Day is \\na particularly dangerous day to drive.\",\n  caption = \"Source: Originally studied by Harper S, Palayew A \\\"The annual cannabis holiday and fatal traffic crashes.\\\"\\nChart produced by Steven Villalon for Tidy Tuesday exercise on April 22, 2025\",\n  theme = theme(text = element_text(family = family),\n                plot.caption.position = \"plot\",\n                plot.caption = element_text(hjust = 0),\n                plot.title = element_text(face = \"bold\"))\n)\n\np1 + p2 + plot_title\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  }
]